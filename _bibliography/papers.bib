---
---

@INPROCEEDINGS{aimen2021stress,
      title={Stress Testing of Meta-learning Approaches for Few-shot Learning}, 
      author={Aroof Aimen and Sahil Sidheekh and Vineet Madan and Narayanan C. Krishnan},
      year={2021},
      booktitle={ 11th International Conference on Computing, Communication and Networking Technologies (ICCCNT)}, 
      pdf={stress_test.pdf},
      url={https://sites.google.com/chalearn.org/metalearning?pli=1#h.kt23ep5wlehv},
      html={https://sites.google.com/chalearn.org/metalearning?pli=1#h.kt23ep5wlehv},
      teaser={stress_test.png},
      selected={true},
}

@article{sidheekh2020duality,
      title={On Duality Gap as a Measure for Monitoring GAN Training}, 
      author={Sahil Sidheekh and Aroof Aimen and Vineet Madan and Narayanan C. Krishnan},
      year={2021},
      abstract={Generative adversarial network (GAN) is among the most popular deep learning models for learning complex data distributions. However, training a GAN is known to be a challenging task. This is often attributed to the lack of correlation between the training progress and the trajectory of the generator and discriminator losses and the need for the GAN's subjective evaluation. A recently proposed measure inspired by game theory - the duality gap, aims to bridge this gap. However, as we demonstrate, the duality gap's capability remains constrained due to limitations posed by its estimation process. This paper presents a theoretical understanding of this limitation and proposes a more dependable estimation process for the duality gap. At the crux of our approach is the idea that local perturbations can help agents in a zero-sum game escape non-Nash saddle points efficiently. Through exhaustive experimentation across GAN models and datasets, we establish the efficacy of our approach in capturing the GAN training progress with minimal increase to the computational complexity. Further, we show that our estimate, with its ability to identify model convergence/divergence, is a potential performance measure that can be used to tune the hyperparameters of a GAN.},
      journal={Under Review. },
      pdf={pert_dg.pdf},
      url={https://arxiv.org/abs/2012.06723},
      html={https://arxiv.org/abs/2012.06723},
      code={https://github.com/perturbed-dg/Perturbed-Duality-Gap},
      teaser={pert_dg.png},
      selected={true},
      
}

@article{sidheekh2021learning,
      title={Learning Neural Networks on SVD Boosted Latent Spaces for Semantic Classification}, 
      author={Sahil Sidheekh},
      year={2021},
      abstract={The availability of large amounts of data and compelling computation power have made deep learning models much popular for text classification and sentiment analysis. Deep neural networks have achieved competitive performance on the above tasks when trained on naive text representations such as word count, term frequency, and binary matrix embeddings. However, many of the above representations result in the input space having a dimension of the order of the vocabulary size, which is enormous. This leads to a blow-up in the number of parameters to be learned, and the computational cost becomes infeasible when scaling to domains that require retaining a colossal vocabulary. This work proposes using singular value decomposition to transform the high dimensional input space to a lower-dimensional latent space. We show that neural networks trained on this lower-dimensional space are not only able to retain performance while savoring significant reduction in the computational complexity but, in many situations, also outperforms the classical neural networks trained on the native input space.},
      journal={Computing Research Repository, ArXiv . },
      pdf={nn_svd.pdf},
      url={https://arxiv.org/abs/2101.00563},
      html={https://arxiv.org/abs/2101.00563},
      code={https://github.com/sahilsid/svd-for-neural-networks},
      teaser={nn_svd.png},
}

@INPROCEEDINGS{9225276,
  author={A. {Aimen} and A. {Kaur} and S. {Sidheekh}},
  booktitle={ 11th International Conference on Computing, Communication and Networking Technologies (ICCCNT)}, 
  title={Scale Invariant Fast PHT based Copy-Move Forgery Detection}, 
  year={2020},
  volume={},
  number={},
  pages={1-7},
  abstract={Copy-Move forgery is a type of image forgery wherein a patch from the image is copied and pasted on the same image either to increase the occurrence of a particular object or to conceal some important detail in the image. This paper addresses the issue of copy-move forgery using the block-based method of feature extraction. In block-based methods of feature extraction, PHT is one of the competing solutions, but it is not much robust to scaling. This paper proposes Scale-Invariant Fast PHT (SIFPHT) algorithm to detect the copy-move forgery which uses Fast PHT [1] for extracting the features from the blocks. Fast PHT has a higher convergence rate than the traditional PHT, and the results prove that the speed-up of almost 4 is attained for detecting the forgery. Moreover, the Fast PHT features so obtained from the blocks are normalized before comparison due to which the scaled forged segments are also identified. Further, Fast K-Means clustering is used to estimate the similarity in the blocks and hence detect the copy-move forgery.},
  keywords={feature extraction;Haar transforms;image segmentation;pattern clustering;image forgery;block-based method;feature extraction;scale invariant fast PHT based copy-move forgery detection;fast PHT features;large-scaled forged segments;fast k-means clustering;SIFPHT;convergence rate;Forgery;Feature extraction;Transforms;Clustering algorithms;Harmonic analysis;Transform coding;Discrete cosine transforms;Copy-Move;PHT;Forgery;Fast PHT;Cluster;Blocks;Features;Images},
  doi={10.1109/ICCCNT49239.2020.9225276},
  ISSN={},
  month={July},
  pdf={pht_cmfd.pdf},
  url={https://ieeexplore.ieee.org/abstract/document/9225276},
  html={https://ieeexplore.ieee.org/abstract/document/9225276},
  teaser={pht_cmfd.png},
  }

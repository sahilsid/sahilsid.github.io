---
---


@article{sidheekh2021characterizing,
      title={Characterizing GAN Convergence Through Proximal Duality Gap}, 
      author={Sahil Sidheekh and Aroof Aimen and Narayanan C. Krishnan},
      year={2021},
      abstract={Despite the accomplishments of Generative Adversarial Networks (GANs) in modeling data distributions, training them remains a challenging task. A contributing factor to this difficulty is the non-intuitive nature of the GAN loss curves, which necessitates a subjective evaluation of the generated output to infer training progress. Recently, motivated by game theory, duality gap has been proposed as a domain agnostic measure to monitor GAN training. However, it is restricted to the setting when the GAN converges to a Nash equilibrium. But GANs need not always converge to a Nash equilibrium to model the data distribution. In this work, we extend the notion of duality gap to proximal duality gap that is applicable to the general context of training GANs where Nash equilibria may not exist. We show theoretically that the proximal duality gap is capable of monitoring the convergence of GANs to a wider spectrum of equilibria that subsumes Nash equilibria. We also theoretically establish the relationship between the proximal duality gap and the divergence between the real and generated data distributions for different GAN formulations. Our results provide new insights into the nature of GAN convergence. Finally, we validate experimentally the usefulness of proximal duality gap for monitoring and influencing GAN training.},
      journal={International Conference on Machine Learning (ICML) },
      pdf={proximal_dg.pdf},
      url={https://arxiv.org/abs/2105.04801},
      html={https://arxiv.org/abs/2105.04801},
      code={https://github.com/proximal-dg/proximal_dg},
      teaser={proximal_dg.png},
      selected={true},
      
}

@INPROCEEDINGS{aimen2021stress,
      title={Stress Testing of Meta-learning Approaches for Few-shot Learning}, 
      author={Aroof Aimen and Sahil Sidheekh and Vineet Madan and Narayanan C. Krishnan},
      year={2021},
      booktitle={ AAAI Workshop on Metalearning }, 
      pdf={stress_test.pdf},
      url={https://sites.google.com/chalearn.org/metalearning?pli=1#h.kt23ep5wlehv},
      html={https://sites.google.com/chalearn.org/metalearning?pli=1#h.kt23ep5wlehv},
      teaser={stress_test.png},
      selected={true},
}

@article{sidheekh2020duality,
      title={On Duality Gap as a Measure for Monitoring GAN Training}, 
      author={Sahil Sidheekh and Aroof Aimen and Vineet Madan and Narayanan C. Krishnan},
      year={2021},
      abstract={Generative adversarial network (GAN) is among the most popular deep learning models for learning complex data distributions. However, training a GAN is known to be a challenging task. This is often attributed to the lack of correlation between the training progress and the trajectory of the generator and discriminator losses and the need for the GAN's subjective evaluation. A recently proposed measure inspired by game theory - the duality gap, aims to bridge this gap. However, as we demonstrate, the duality gap's capability remains constrained due to limitations posed by its estimation process. This paper presents a theoretical understanding of this limitation and proposes a more dependable estimation process for the duality gap. At the crux of our approach is the idea that local perturbations can help agents in a zero-sum game escape non-Nash saddle points efficiently. Through exhaustive experimentation across GAN models and datasets, we establish the efficacy of our approach in capturing the GAN training progress with minimal increase to the computational complexity. Further, we show that our estimate, with its ability to identify model convergence/divergence, is a potential performance measure that can be used to tune the hyperparameters of a GAN.},
      journal={International Joint Conference on Neural Networks (IJCNN) },
      pdf={pert_dg.pdf},
      url={https://arxiv.org/abs/2012.06723},
      html={https://arxiv.org/abs/2012.06723},
      code={https://github.com/perturbed-dg/Perturbed-Duality-Gap},
      teaser={pert_dg.png},
      selected={true},
      
}

@article{sidheekh2021learning,
      title={Learning Neural Networks on SVD Boosted Latent Spaces for Semantic Classification}, 
      author={Sahil Sidheekh},
      year={2021},
      abstract={The availability of large amounts of data and compelling computation power have made deep learning models much popular for text classification and sentiment analysis. Deep neural networks have achieved competitive performance on the above tasks when trained on naive text representations such as word count, term frequency, and binary matrix embeddings. However, many of the above representations result in the input space having a dimension of the order of the vocabulary size, which is enormous. This leads to a blow-up in the number of parameters to be learned, and the computational cost becomes infeasible when scaling to domains that require retaining a colossal vocabulary. This work proposes using singular value decomposition to transform the high dimensional input space to a lower-dimensional latent space. We show that neural networks trained on this lower-dimensional space are not only able to retain performance while savoring significant reduction in the computational complexity but, in many situations, also outperforms the classical neural networks trained on the native input space.},
      journal={Computing Research Repository, ArXiv . },
      pdf={nn_svd.pdf},
      url={https://arxiv.org/abs/2101.00563},
      html={https://arxiv.org/abs/2101.00563},
      code={https://github.com/sahilsid/svd-for-neural-networks},
      teaser={nn_svd.png},
}

@INPROCEEDINGS{9225276,
  author={A. {Aimen} and A. {Kaur} and S. {Sidheekh}},
  booktitle={ 11th International Conference on Computing, Communication and Networking Technologies (ICCCNT)}, 
  title={Scale Invariant Fast PHT based Copy-Move Forgery Detection}, 
  year={2020},
  volume={},
  number={},
  pages={1-7},
  abstract={Copy-Move forgery is a type of image forgery wherein a patch from the image is copied and pasted on the same image either to increase the occurrence of a particular object or to conceal some important detail in the image. This paper addresses the issue of copy-move forgery using the block-based method of feature extraction. In block-based methods of feature extraction, PHT is one of the competing solutions, but it is not much robust to scaling. This paper proposes Scale-Invariant Fast PHT (SIFPHT) algorithm to detect the copy-move forgery which uses Fast PHT [1] for extracting the features from the blocks. Fast PHT has a higher convergence rate than the traditional PHT, and the results prove that the speed-up of almost 4 is attained for detecting the forgery. Moreover, the Fast PHT features so obtained from the blocks are normalized before comparison due to which the scaled forged segments are also identified. Further, Fast K-Means clustering is used to estimate the similarity in the blocks and hence detect the copy-move forgery.},
  keywords={feature extraction;Haar transforms;image segmentation;pattern clustering;image forgery;block-based method;feature extraction;scale invariant fast PHT based copy-move forgery detection;fast PHT features;large-scaled forged segments;fast k-means clustering;SIFPHT;convergence rate;Forgery;Feature extraction;Transforms;Clustering algorithms;Harmonic analysis;Transform coding;Discrete cosine transforms;Copy-Move;PHT;Forgery;Fast PHT;Cluster;Blocks;Features;Images},
  doi={10.1109/ICCCNT49239.2020.9225276},
  ISSN={},
  month={July},
  pdf={pht_cmfd.pdf},
  url={https://ieeexplore.ieee.org/abstract/document/9225276},
  html={https://ieeexplore.ieee.org/abstract/document/9225276},
  teaser={pht_cmfd.png},
  }

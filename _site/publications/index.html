<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Sahil  Sidheekh | publications</title>
<meta name="description" content="">

<meta name="google-site-verification" content="1xyFU2bnNPqIhDpt5n9mzGHa9J83Vla2AAmsPzakIP0" />
<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/publications/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

  <script src="/assets/js/theme.js"></script>
  <!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>



  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-XXXXXXXXX"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'UA-XXXXXXXXX');
  </script>



<!-- Panelbear Analytics - We respect your privacy -->
<script async src="https://cdn.panelbear.com/analytics.js?site=XXXXXXXXX"></script>
<script>
    window.panelbear = window.panelbear || function() { (window.panelbear.q = window.panelbear.q || []).push(arguments); };
    panelbear('config', { site: 'XXXXXXXXX' });
</script>


    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="http://localhost:4000/">
       <span class="font-weight-bold">Sahil</span>   Sidheekh
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item active">
              <a class="nav-link" href="/publications/">
                
                publications
                
                
                
                <span class="sr-only">(current)</span>
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                
                projects
                
                
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/service/">
                
                service
                
                
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/personal/">
                
                <span style="color: lightslategrey;">personal</span>
                
                
                
              </a>
          </li>
          
          
          
            <div class="toggle-container">
              <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">publications</h1>
    <p class="post-description"></p>
  </header>

  <article>
    <div class="publications">


  <h2 class="year">2023</h2>
  <ol class="bibliography"><li>
<div class="row">
  <div class="col-sm-2 col-6">
  
    
    <abbr class="badge">UAI 2023 (Oral)</abbr>
    
  
  
        <img class="img-fluid z-depth-1 rounded" src="/assets/img/teaser/pfc.png">
      
  </div>

  <div id="sidheekh2023probabilistic" class="col-sm-8">
    
      <div class="title">Probabilistic Flow Circuits: Towards Unified Deep Models for Tractable Probabilistic Inference</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Sahil Sidheekh</em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://ml-research.github.io/people/kkersting/" target="_blank" rel="noopener noreferrer">Kristian Kersting</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="https://utdallas.edu/~sriraam.natarajan" target="_blank" rel="noopener noreferrer">Sriraam Natarajan</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In The 39th Conference on Uncertainty in Artificial Intelligence (UAI)</em>
      
      
        2023
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
    
      
      <a href="/assets/pdf/pfc.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/sahilsid/probabilistic-flow-circuits" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
      
      <a href="/assets/pdf/pfc-poster.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a>
      
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We consider the problem of increasing the expressivity of probabilistic circuits by augmenting them with the successful generative models of normalizing flows. To this effect, we theoretically establish the requirement of decomposability for such combinations to retain tractability of the learned models. Our model, called Probabilistic Flow Circuits, essentially extends circuits by allowing for normalizing flows at the leaves. Our empirical evaluation clearly establishes the expressivity and tractability of this new class of probabilistic circuits</p>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2022</h2>
  <ol class="bibliography">
<li>
<div class="row">
  <div class="col-sm-2 col-6">
  
    
    <abbr class="badge">UAI 2022</abbr>
    
  
  
        <img class="img-fluid z-depth-1 rounded" src="/assets/img/teaser/vqflows.png">
      
  </div>

  <div id="sidheekh2022vqflows" class="col-sm-8">
    
      <div class="title">VQ-Flows: Vector Quantized Local Normalizing Flows</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Sahil Sidheekh</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                 Chris Barton Dock,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                 Tushar Jain,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                 Radu Balan,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Maneesh Kumar Singh,
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In The 38th Conference on Uncertainty in Artificial Intelligence (UAI)</em>
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
      <a href="https://openreview.net/forum?id=rn4xnDUjcl9" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
    
    
      
      <a href="/assets/pdf/vqflows.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
      
      <a href="/assets/pdf/vqflows-poster.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a>
      
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Normalizing flows provide an elegant approach to generative modeling that allows for efficient sampling and exact density evaluation of unknown data distributions. However, current techniques have significant limitations in their expressivity when the data distribution is supported on a low-dimensional manifold or has a non-trivial topology. We introduce a novel statistical framework for learning a mixture of local normalizing flows as "chart maps" over the data manifold. Our framework augments the expressivity of recent approaches while preserving the signature property of normalizing flows, that they admit exact density evaluation. We learn a suitable atlas of charts for the data manifold via a vector quantized auto-encoder (VQ-AE) and the distributions over them using a conditional flow. We validate experimentally that our probabilistic framework enables existing approaches to better model data distributions over complex manifolds.</p>
    </div>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 col-6">
  
    
    <abbr class="badge">RSOS 2022</abbr>
    
  
  
        <img class="img-fluid z-depth-1 rounded" src="/assets/img/teaser/ewsnet.png">
      
  </div>

  <div id="deb2021machine" class="col-sm-8">
    
      <div class="title">Machine learning methods trained on simple models can predict critical transitions in complex natural systems</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                 Smita Deb,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Sahil Sidheekh</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                 Christopher F. Clements,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://seekayan.github.io/ck.html" target="_blank" rel="noopener noreferrer">Narayanan C. Krishnan</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Partha S. Dutta,
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Royal Society Open Science Journal, </em>
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
      <a href="https://www.biorxiv.org/content/10.1101/2021.03.15.435556v1" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
    
    
      
      <a href="/assets/pdf/ewsnet.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/sahilsid/EWSNet" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>1. Sudden transitions from one stable state to a contrasting state occur in complex systems ranging from the collapse of ecological populations to climatic change, with much recent work seeking to develop methods to predict these unexpected transitions from signals in time series data. However, previously developed methods vary widely in their reliability, and fail to classify whether an approaching collapse might be catastrophic (and hard to reverse) or non-catastrophic (easier to reverse) with significant implications for how such systems are managed.
      2. Here we develop a novel detection method, using simulated outcomes from a range of simple mathematical models with varying nonlinearity to train a deep neural network to detect critical transitions - the Early Warning Signal Network (EWSNet).
      3. We demonstrate that this neural network (EWSNet), trained on simulated data with minimal assumptions about the underlying structure of the system, can predict with high reliability observed real-world transitions in ecological and climatological data. Importantly, our model appears to capture latent properties in time series missed by previous warning signals approaches, allowing us to not only detect if a transition is approaching but critically whether the collapse will be catastrophic or non-catastrophic.
      4. The EWSNet can flag a critical transition with unprecedented accuracy, overcoming some of the major limitations of traditional methods based on phenomena such as Critical Slowing Down. These novel properties mean EWSNet has the potential to serve as a universal indicator of transitions across a broad spectrum of complex systems, without requiring information on the structure of the system being monitored. Our work highlights the practicality of deep learning for addressing further questions pertaining to ecosystem collapse and have much broader management implications.</p>
    </div>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 col-6">
  
  
        <img class="img-fluid z-depth-1 rounded" src="/assets/img/teaser/acf.png">
      
  </div>

  <div id="mukherjee2021attentive" class="col-sm-8">
    
      <div class="title">Attentive Contractive Flow: Improved Contractive Flows with Lipschitz-constrained Self-Attention</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                 Avideep Mukherjee,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                 Badri Narayan Patro,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Sahil Sidheekh</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                 Maneesh Singh,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Vinay P. Namboodiri,
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Preprint, Under Review</em>
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Normalizing flows provide an elegant method for obtaining tractable density estimates from distributions by using invertible transformations. The main challenge is to improve the expressivity of the models while keeping the invertibility constraints intact. We propose to do so via the incorporation of localized self-attention. However, conventional self-attention mechanisms don’t satisfy the requirements to obtain invertible flows and can’t be naively incorporated into normalizing flows. To address this, we introduce a novel approach called Attentive Contractive Flow (ACF) which utilizes a special category of flow-based generative models - contractive flows. We demonstrate that ACF can be introduced into a variety of state of the art flow models in a plug-and-play manner. This is demonstrated to not only improve the representation power of these models (improving on the bits per dim metric), but also to results in significantly faster convergence in training them. Qualitative results, including interpolations between test images, demonstrate that samples are more realistic and capture local correlations in the data well. We evaluate the results further by performing perturbation analysis using AWGN demonstrating that ACF models (especially the dot-product variant) show better and more consistent resilience to additive noise.</p>
    </div>
    
  </div>
</div>
</li>
</ol>

  <h2 class="year">2021</h2>
  <ol class="bibliography">
<li>
<div class="row">
  <div class="col-sm-2 col-6">
  
    
    <abbr class="badge">ICML 2021 </abbr>
    
  
  
        <img class="img-fluid z-depth-1 rounded" src="/assets/img/teaser/proximal_dg.png">
      
  </div>

  <div id="sidheekh2021characterizing" class="col-sm-8">
    
      <div class="title">On Characterizing GAN Convergence Through Proximal Duality Gap</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Sahil Sidheekh</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                 Aroof Aimen,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="https://seekayan.github.io/ck.html" target="_blank" rel="noopener noreferrer">Narayanan C. Krishnan</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the 38th International Conference on Machine Learning (ICML) </em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
      <a href="https://icml.cc/virtual/2021/poster/8559" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
    
    
      
      <a href="/assets/pdf/proximal_dg.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/proximal-dg/proximal_dg" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Despite the accomplishments of Generative Adversarial Networks (GANs) in modeling data distributions, training them remains a challenging task. A contributing factor to this difficulty is the non-intuitive nature of the GAN loss curves, which necessitates a subjective evaluation of the generated output to infer training progress. Recently, motivated by game theory, duality gap has been proposed as a domain agnostic measure to monitor GAN training. However, it is restricted to the setting when the GAN converges to a Nash equilibrium. But GANs need not always converge to a Nash equilibrium to model the data distribution. In this work, we extend the notion of duality gap to proximal duality gap that is applicable to the general context of training GANs where Nash equilibria may not exist. We show theoretically that the proximal duality gap is capable of monitoring the convergence of GANs to a wider spectrum of equilibria that subsumes Nash equilibria. We also theoretically establish the relationship between the proximal duality gap and the divergence between the real and generated data distributions for different GAN formulations. Our results provide new insights into the nature of GAN convergence. Finally, we validate experimentally the usefulness of proximal duality gap for monitoring and influencing GAN training.</p>
    </div>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 col-6">
  
    
    <abbr class="badge">NeurIPS-W 2021</abbr>
    
  
  
        <img class="img-fluid z-depth-1 rounded" src="/assets/img/teaser/taskatt.png">
      
  </div>

  <div id="aimen2021task" class="col-sm-8">
    
      <div class="title">Task Attended Meta-Learning for Few-Shot Learning</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                 Aroof Aimen,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Sahil Sidheekh</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                 Bharat Ladrecha,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="https://seekayan.github.io/ck.html" target="_blank" rel="noopener noreferrer">Narayanan Chatapuram Krishnan</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Fifth Workshop on Meta-Learning at the Conference on Neural Information Processing Systems (NeurIPS)</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
      <a href="https://openreview.net/forum?id=D380IGEhAnF" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
    
    
      
      <a href="/assets/pdf/task_att_neurips.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/taskattention/task-attended-metalearning.git" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Meta-learning (ML) has emerged as a promising direction in learning models under constrained resource settings like few-shot learning. The popular approaches for ML either learn a generalizable initial model or a generic parametric optimizer through episodic training. The former approaches leverage the knowledge from a batch of tasks to learn an optimal prior. In this work, we study the importance of a batch for ML. Specifically, we first incorporate a batch episodic training regimen to improve the learning of the generic parametric optimizer. We also hypothesize that the common assumption in batch episodic training that each task in a batch has an equal contribution to learning an optimal meta-model need not be true. We propose to weight the tasks in a batch according to their "importance" in improving the meta-model’s learning. To this end, we introduce a training curriculum motivated by selective focus in humans, called task attended meta-training, to weight the tasks in a batch. Task attention is a standalone module that can be integrated with any batch episodic training regimen. The comparisons of the models with their non-task-attended counterparts on complex datasets like miniImageNet and tieredImageNet validate its effectiveness.</p>
    </div>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 col-6">
  
    
    <abbr class="badge">AAAI-W 2021</abbr>
    
  
  
        <img class="img-fluid z-depth-1 rounded" src="/assets/img/teaser/stress_test.png">
      
  </div>

  <div id="aimen2021stress" class="col-sm-8">
    
      <div class="title">Stress Testing of Meta-learning Approaches for Few-shot Learning</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                 Aroof Aimen,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Sahil Sidheekh</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                 Vineet Madan,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="https://seekayan.github.io/ck.html" target="_blank" rel="noopener noreferrer">Narayanan C. Krishnan</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In  AAAI Workshop on Metalearning and Co-Hosted Challenge</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
    
    
      <a href="https://sites.google.com/chalearn.org/metalearning?pli=1#h.kt23ep5wlehv" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
    
    
      
      <a href="/assets/pdf/stress_test.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 col-6">
  
    
    <abbr class="badge">IJCNN 2021</abbr>
    
  
  
        <img class="img-fluid z-depth-1 rounded" src="/assets/img/teaser/pert_dg.png">
      
  </div>

  <div id="sidheekh2020duality" class="col-sm-8">
    
      <div class="title">On Duality Gap as a Measure for Monitoring GAN Training</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Sahil Sidheekh</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                 Aroof Aimen,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                 Vineet Madan,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="https://seekayan.github.io/ck.html" target="_blank" rel="noopener noreferrer">Narayanan C. Krishnan</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In International Joint Conference on Neural Networks (IJCNN) </em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
      <a href="https://arxiv.org/abs/2012.06723" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
    
    
      
      <a href="/assets/pdf/pert_dg.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/perturbed-dg/Perturbed-Duality-Gap" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Generative adversarial network (GAN) is among the most popular deep learning models for learning complex data distributions. However, training a GAN is known to be a challenging task. This is often attributed to the lack of correlation between the training progress and the trajectory of the generator and discriminator losses and the need for the GAN’s subjective evaluation. A recently proposed measure inspired by game theory - the duality gap, aims to bridge this gap. However, as we demonstrate, the duality gap’s capability remains constrained due to limitations posed by its estimation process. This paper presents a theoretical understanding of this limitation and proposes a more dependable estimation process for the duality gap. At the crux of our approach is the idea that local perturbations can help agents in a zero-sum game escape non-Nash saddle points efficiently. Through exhaustive experimentation across GAN models and datasets, we establish the efficacy of our approach in capturing the GAN training progress with minimal increase to the computational complexity. Further, we show that our estimate, with its ability to identify model convergence/divergence, is a potential performance measure that can be used to tune the hyperparameters of a GAN.</p>
    </div>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 col-6">
  
  
        <img class="img-fluid z-depth-1 rounded" src="/assets/img/teaser/nn_svd.png">
      
  </div>

  <div id="sidheekh2021learning" class="col-sm-8">
    
      <div class="title">Learning Neural Networks on SVD Boosted Latent Spaces for Semantic Classification</div>
      <div class="author">
        
          
          
          
          
          
          
            
              <em>Sahil Sidheekh</em>
            
          
        
      </div>

      <div class="periodical">
      
        <em>Computing Research Repository, ArXiv . </em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
      <a href="https://arxiv.org/abs/2101.00563" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
    
    
      
      <a href="/assets/pdf/nn_svd.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/sahilsid/svd-for-neural-networks" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The availability of large amounts of data and compelling computation power have made deep learning models much popular for text classification and sentiment analysis. Deep neural networks have achieved competitive performance on the above tasks when trained on naive text representations such as word count, term frequency, and binary matrix embeddings. However, many of the above representations result in the input space having a dimension of the order of the vocabulary size, which is enormous. This leads to a blow-up in the number of parameters to be learned, and the computational cost becomes infeasible when scaling to domains that require retaining a colossal vocabulary. This work proposes using singular value decomposition to transform the high dimensional input space to a lower-dimensional latent space. We show that neural networks trained on this lower-dimensional space are not only able to retain performance while savoring significant reduction in the computational complexity but, in many situations, also outperforms the classical neural networks trained on the native input space.</p>
    </div>
    
  </div>
</div>
</li>
</ol>

  <h2 class="year">2020</h2>
  <ol class="bibliography"><li>
<div class="row">
  <div class="col-sm-2 col-6">
  
  
        <img class="img-fluid z-depth-1 rounded" src="/assets/img/teaser/pht_cmfd.png">
      
  </div>

  <div id="9225276" class="col-sm-8">
    
      <div class="title">Scale Invariant Fast PHT based Copy-Move Forgery Detection</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                 Aroof Aimen,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                 Amandeep Kaur,
                
              
            
          
        
          
          
          
          
          
          
            
              
                and <em>Sahil Sidheekh</em>,
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In  11th International Conference on Computing, Communication and Networking Technologies (ICCCNT)</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
      <a href="https://ieeexplore.ieee.org/abstract/document/9225276" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
    
    
      
      <a href="/assets/pdf/pht_cmfd.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Copy-Move forgery is a type of image forgery wherein a patch from the image is copied and pasted on the same image either to increase the occurrence of a particular object or to conceal some important detail in the image. This paper addresses the issue of copy-move forgery using the block-based method of feature extraction. In block-based methods of feature extraction, PHT is one of the competing solutions, but it is not much robust to scaling. This paper proposes Scale-Invariant Fast PHT (SIFPHT) algorithm to detect the copy-move forgery which uses Fast PHT [1] for extracting the features from the blocks. Fast PHT has a higher convergence rate than the traditional PHT, and the results prove that the speed-up of almost 4 is attained for detecting the forgery. Moreover, the Fast PHT features so obtained from the blocks are normalized before comparison due to which the scaled forged segments are also identified. Further, Fast K-Means clustering is used to estimate the similarity in the blocks and hence detect the copy-move forgery.</p>
    </div>
    
  </div>
</div>
</li></ol>


</div>

  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    © Copyright 2023 Sahil  Sidheekh.
    Built using <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a>.

    
    
    Last updated: December 25, 2023.
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
